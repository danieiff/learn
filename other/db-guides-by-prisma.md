## Compare Database types

Hierarchies with few interconnections, raw structured documents or objects with at least some fields in common	Document databases (Couchbase, MongoDB)
Huge volumes of "flat" data such as instrument readings or structured analytics	Column-oriented databases (Cassandra, HBase), time-series databases (TimescaleDB, InfluxDB, Druid)
Records identified by a key and queried only by that key	Key-value stores (Redis, LevelDB), caches (memcached)
Complex relationships among records with point-to-point navigation	Graph databases (OrientDB, Neo4j, TerminusDB)
Transient data feeds in the order of receipt	Stream processors and queues (Kafka, RabbitMQ, ZeroMQ)
Files. Lots and lots of files	Cloud file storage (Google Cloud Storage, Amazon S3 and Athena)
Unstructured documents, or structured documents with highly variable schemas	Search engines (ElasticSearch, Solr), content repositories (Jackrabbit)
Relational data, at planetary scale	NewSQL highly-distributed relational databases (VoltDB, Spanner)
At least several of the above	Multi-model databases (FaunaDB, ArangoDB)

NewSQL: Spanner, Calvin, CockroachDB, FaunaDB, yugabyteDB, PlanetScale

https://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis


## Maybe read later?

https://www.prisma.io/dataguide/types/relational/infrastructure-architecture#serverless-databases

PostgreSQL
MySQL
SQLite
MongoDB

https://www.prisma.io/dataguide/database-tools/connection-pooling
https://www.prisma.io/dataguide/managing-databases/database-troubleshooting
https://www.prisma.io/dataguide/managing-databases/how-to-spot-bottlenecks-in-performance
https://www.prisma.io/dataguide/managing-databases/syncing-development-databases-between-team-members
https://www.prisma.io/dataguide/managing-databases/database-replication/database-replication-introduction
https://www.prisma.io/dataguide/managing-databases/introduction-to-OLAP-OLTP
https://www.prisma.io/dataguide/managing-databases/microservices-vs-monoliths
https://www.prisma.io/dataguide/managing-databases/introduction-database-caching
https://www.prisma.io/dataguide/managing-databases/testing-in-production
https://www.prisma.io/dataguide/managing-databases/backup-considerations
https://www.prisma.io/dataguide/managing-databases/intro-to-full-text-search


## Glossary (few of https://www.prisma.io/dataguide/intro/database-glossary)

<dl><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="1nf"><strong>1NF</strong></dt><dd>1NF, or first normal form, describes a type of database normalization where each table column only has a single value.  A column that has a nested table as a value or multiple values is not in 1NF.</dd></div>
  <div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="2nf"><strong>2NF</strong></dt><dd>2NF, or second normal form, describes a type of database normalization that: 1) satisfies the requirements of 1NF, 2) has no values that are tied directly to a subset of a candidate key.  In other words, a relation is in 2NF if it is in 1NF and all of the non-candidate values are dependent on the composite key in whole, not just a portion of the candidate key.  For example, a `book` table that has a candidate key composed of `title` and `author` cannot be in 2NF if it also includes a `dob` field describing the author&#x27;s date of birth.  That column value is dependent only on the value of `author` and could lead to inconsistencies if the values get out of sync.</dd></div>
  <div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="3nf"><strong>3NF</strong></dt><dd>3NF, or third normal form, describes a type of database normalization that: 1) satisfies the requirements of 2NF, 2) each non-key attribute is not transitively dependent on a key attribute.  For example, if a `user` table has a `user_id` column as a primary key, a `user_city` column, and a `user_state` column, it would not be in 3NF because `user_state` is transitively dependent on `user_id` through `user_city` (the city and state should be extracted to their own table and referenced together).</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="4nf"><strong>4NF</strong></dt><dd>4NF, or fourth normal form, describes a type of database normalization that: 1) satisfies the requirements of BCNF, 2) for every non-trivial multivalued dependency, the determining attribute in the dependency is either a candidate key or a superset of it.  In other words, if a field has multiple dependent fields that are independent from one another, it can lead to redundancies that violate 4NF rules.</dd></div>
  <div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="acid"><strong>ACID</strong></dt><dd>ACID — an acronym created from the words atomicity, consistency, isolation, and durability — describes a set of characteristics that database transactions are meant to provide.  Atomicity guarantees that all operations in a transaction will complete successfully or will be rolled back.  Consistency, often considered a property maintained by the application rather than the database, is often achieved through transactions to make sure that all related values are updated at once.  Transaction isolation aims to allow simultaneous transactions to execute independently.  Durability means that transactions are meant to be stored on non-volatile storage when committed.</dd></div>
  <div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="anti-caching"><strong>Anti-caching</strong></dt><dd>Anti-caching is a strategy that can be used when data is not found in the faster in-memory cache and must be retrieved from slower, persistent storage.  The technique involves aborting the transaction and kicking off an asynchronous operation to fetch the data from the slower medium to memory.  The transaction can be retried later and the information will be ready to served from memory.</dd></div>
  <div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="atomicity"><strong>Atomicity</strong></dt><dd>Atomicity is a quality mainly associated with database transactions that means that the operations encapsulated in the transaction are handled in an all-or-nothing fashion.  This prevents partial updates from occurring where some operations were performed before an error condition arose, leading to inconsistent data.  In the case of transactions, either all of the operations are committed or every operation is rolled back to leave the database in the same state that it was in when the transaction began.</dd></div>
  <div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="attributes"><strong>Attributes</strong></dt><dd>Attributes are characteristics that describe a certain entity in a database.  In the ER (entity-relationship) model, attributes are any additional properties that are not relationships that add information about an entity.</dd></div>
    strong>BASE</strong></dt><dd>BASE —  an acronym created from the words Basically Available, Soft-state, and Eventually consistent — describes a set of characteristics of some NoSQL databases.  It is offered as an description for certain databases that do not conform to the properties described by ACID-compliance (atomicity, consistency, isolation, and durability).  BASE databases choose to remain available at the expense of strict data consistency in cases of network partitions.  The soft-state component refers to the fact that the state of the system can be in flux as the different members negotiate the most correct values in the system.  Eventually consistent is another related statement indicating that the system will eventually achieve consistency given enough time and assuming new inconsistencies aren&#x27;t introduced during that time.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="bcnf"><
    strong>BCNF</strong></dt><dd>BCNF, or Boyce-Codd normal form, describes a type of database normalization that: 1) satisfies the requirements of 3NF, 2) where the determining attribute in each dependency (the attribute that dictates another attribute&#x27;s value) is either a _superset_ of the dependent attribute, is a candidate key, or is a superset of a candidate key.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="blue-green-deployments"><
    strong>Blue-green deployments</strong></dt><dd>Blue-green deployments are a technique for deploying software updates with little to no downtime by managing active traffic between two identical sets of infrastructure.  New releases can be deployed to the inactive infrastructure group and tested independently.  To release the new version, a traffic routing mechanism is switched to direct traffic from the current infrastructure to the infrastructure with the new version.  The previously-active infrastructure now functions as the target for the next updates.  This strategy is helpful in that the routing mechanism can easily switch back and forth to roll backwards or forwards depending on the success of a deployment.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="bottleneck"><
    strong>Cache-aside</strong></dt><dd>Cache-aside is a caching architecture that positions the cache outside of the regular path between application and database.  In this arrangement, the application will fetch data from the cache if it is available there.  If the data is not in the cache, the application will issue a separate query to the original data source to fetch the data and then write that data to the cache for subsequent queries.  The minimal crossover between the cache and backing data source allows this architecture to be resilient against unavailable caches.  Cache-aside is well-suited for read-heavy workloads.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="cache-invalidation"><
    strong>Cache invalidation</strong></dt><dd>Cache invalidation is the process of targeting and removing specific items from a cache.  Most often, this is performed as part of a routine when updating records so that the data in the cache does not serve stale data to clients.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="canary-releases"><
    strong>Canary releases</strong></dt><dd>A canary release describes a release strategy where new versions of software are deployed to a small subset of servers to test new changes in an environment with limited impact.  The deployment and resulting behavior of the test group are observed and the team can then decide if they want to roll back the changes or continue to deploy the changes to a wider range of hosts.  Canary releases are a way of testing in production while limiting the number of clients impacted by any problems.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="candidate-key"><
    strong>Candidate key</strong></dt><dd>A candidate key in a relational database is the term for a minimal superkey.  In other words, a candidate key is any column or combination of columns that can be used to uniquely identify each record in a relation without including columns that do not help in specificity.  In a `cars` table, a unique `car_id` column would be a candidate key as well as a combination of the `make`, `model`, and `year` columns (assuming that&#x27;s specific enough to eliminate any duplicates).  However, `car_id` and `make` would not be a candidate key since in this instance, `make` does nothing to narrow down the uniqueness of each row.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="cascade"><
    strong>Cascade</strong></dt><dd>In relational databases, cascade is an option for how to handle deletes or updates for records that have related entries in other tables.  Cascade means that the operation (delete or update) should be applied to the child, dependent rows as well.  This helps you avoid orphaned rows in the case of deletes and out of sync values in the case of updates.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="cassandra"><
    strong>Cassandra</strong></dt><dd>Apache Cassandra is a distributed, wide-column NoSQL database focused on operating on and managing large volumes of data.  Cassandra scales incredibly well and each node in the cluster can accept reads or writes.  Data is stored in rows that are uniquely identifiable and partitioned based on partition key.  Each partition key returns a row of data with both column names and values defined internally, meaning each row in the same column family may contain different columns.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="check-constraint"><
    strong>Check constraint</strong></dt><dd>A check constraint is perhaps the most flexible table or column constraint that can be added to a relational database.  It is defined as a boolean condition that must be met for the proposed data to be accepted by the system.  Because of the nature of the condition is fairly open-ended, check constraints can be used to model many different types of requirements to ensure that the data coming into the system conforms to expectations.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="cluster"><
    strong>Cluster</strong></dt><dd>In computing, a cluster is a group of computers all dedicated to helping with a shared task.  Database clusters are used to increase the capacity, availability, and performance of certain types of actions compared to database deployed on a single computer.  There are many different topologies, technologies, and trade-offs that different clustered systems employ to achieve different levels of performance or fault tolerance.  Because of the diversity of different implementations, it can be difficult to generalize specific characteristics that apply to all clustered database systems.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="collation"><
    strong>Collation</strong></dt><dd>Collation in databases refers to the ordering and comparison characteristics of different character systems.  Most databases allow you to assign collation settings, which impact how text in the system are sorted, displayed, and compared against one another.  Collation is often defined using a set of labels that describe the character set, language context, and different options about sensitivity or insensitivity to capitalization, accents, and other character modifiers.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="collections"><
    strong>Column database</strong></dt><dd>A column database or column-oriented database is a table-oriented database similar to a traditional relational database that stores data in the background by column instead of by record.  This means that the data associated with a single column are stored together rather than grouping all of the data associated with a single record.  This can provide different performance characteristics depending on usage patterns, but generally doesn&#x27;t affect how the user interacts with the data in the table on a daily basis.  Although often confused in the literature, column databases are not to be confused with wide column databases or column family databases.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="column-family"><
    strong>Column family</strong></dt><dd>A column family is a database object that stores groups of key-value pairs where each key is a row identifier and each value is a group of column names and values.  All together, a column family constructs something that is akin to a table in relational databases.  However, each row can define its own columns, meaning that rows are of varying lengths and do not have to match each other in the columns represented or the data types stored.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="command-query-responsibility-segregation"><
    strong>Command query responsibility segregation</strong></dt><dd>Command query responsibility segregation is a application design pattern that allows you to separate operations based on their impact on the underlying database.  In general, this usually means providing different mechanisms for queries that read data versus queries that change data.  Separating these two contexts allows you to make infrastructure and system changes to scale each use-case independently, increasing performance.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="commit"><
    strong>Commit</strong></dt><dd>In the context of databases, committing data is the process whereby you execute and durably store a set of proposed actions.  Many databases are configured to automatically commit each statement as it is received by the system, but transactions, for example, are one mechanism through which you can control the commit behavior of the database by grouping multiple statements together and committing them as a group.  Committing in database is the action that is actually responsible for performing a permanent action on the system.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="composite-key"><
    strong>Composite key</strong></dt><dd>In relational databases, a composite key is a key composed of two or more columns that can be used to uniquely identify any record in a table.  For example, if we have a `shirts` table that only stores a single record for each combination of size and color could have a composite key defined by a combination of the `color` and `size` columns.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="concurrency"><
    strong>Connection pooling</strong></dt><dd>Connection pooling is a strategy used to improve performance and avoid connection exhaustion by managing the connections between an application and database.  It does this by maintaining a pool of connections to the database.  By keeping the connections open and reusing them for multiple queries, the application can forgo the overhead of having to establish a connection each time and the database&#x27;s connection limits can be managed by pooling component.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="consistency"><
    strong>Constraint</strong></dt><dd>A constraint is a limitation imposed on a specific column or table that impacts the range of values accepted by the system.  Constraints are used to define rules that the database system can enforce to ensure that values conform to requirements.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="cursor"><
    strong>Cursor</strong></dt><dd>A database cursor is a way for clients to iterate over records and query results in a controlled, precise manner.  Cursors are primarily used to page through results that match a query one-by-one by iteratively returning the next row for processing.  This can help you operate on an unknown number of records by accessing the results as a queue.  Care must be taken when using cursors as they take up resources on the database system, can result in locking, and often result in many more network round trips than would be required otherwise.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="dark-launching"><
    strong>Dark launching</strong></dt><dd>Dark launching is a deployment and release strategy that helps organizations test new changes in production contexts without affecting the user experience.  Dark launching involves releasing new code in parallel to the original functionality.  Requests and actions are then mirrored and run against both the old code and the new code.  While the system&#x27;s behavior from the user&#x27;s perspective is only affected by the original code, the new code can be tested with real data to validate functionality and catch performance and functional problems.  When properly vetted, the application can be altered to use the new code path exclusively.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="data"><
    strong>Data</strong></dt><dd>In the broadest sense, data are facts or pieces of information.  They are measurements or values that contain information about something.  In some contexts, data is defined as distinct from information in that information is analyzed or processed data while data consists only of raw values.  Practically speaking, however, these terms are often used as synonyms and typically encapsulate any fact along with the relevant context necessary to interpret or contextualize it.  Data is an essential component of almost all communication and activity and it can gain meaning and value as it is collected, analyzed, and contextualized.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="data-definition-language"><
    strong>Data definition language (DDL)</strong></dt><dd>A data definition language, or DDL, is a set of commands or actions that are used to define database structures and objects.  They are a key component to relational and other databases and are expressed as a subset of the available commands available to manage data in languages like SQL.  Data definition language is the portion of the language dedicated to describing, creating, and modifying structures and the frameworks that will hold data.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="data-independence"><
    strong>Data independence</strong></dt><dd>Data independence is a term used to describe the separation of database clients or applications from the underlying structure responsible for representing and storing the data.  Data independence is achieved if the database is able to abstract the structure in a way that allows user applications to continue running even if additional attributes are added to a relation (logical independence) or if the details of the storage medium changes (physical independence), for instance.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="data-mapper-orm"><
    strong>Data type</strong></dt><dd>A data type is a category or attribute that expresses a constraint on valid values.  For example, an integer type specifies that only whole numbers are appropriate and expected for a variable or field.  Data types allow you to specify expectations and requirements about your data during when defining a field or container.  The programming language or application can then validate that introduced data meets the necessary criteria.  Data types are also help determine the available operations that can be performed on a piece of data.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="database"><
    strong>Database</strong></dt><dd>A database is a structure used to organize, structure, and store data.  Databases are often managed by a database management system which provides an interface to manipulate and interact with the database and the data it manages.  Databases can be highly structured or allow more flexible data storage patterns and can store many different types of data in a way that allows for querying, recalling, and combining data at the time of retrieval.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="database-abstraction-layer"><
    strong>Database abstraction layer</strong></dt><dd>A database abstraction layer is a programming interface that attempts to abstract differences between underlying database technologies to provide a unified experience or interface to the application layer.  Database abstraction layers are often helpful for developers because they help to normalize the implementation differences between various offerings and can stay stable even as the underlying technology evolves.  However, there are some challenges as well, such as leaking abstractions, masking implementation-specific features or optimizations from the user, and creating a dependency that can be difficult to dislodge.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="database-administrator"><
    strong>Database administrator (DBA)</strong></dt><dd>A database administrator, or DBA, is a role responsible for configuring, managing, and optimizing database systems and the related ecosystem of software and hardware.  Some responsibilities they may be involved with include architecture planning, configuration, schema and change management, migrations, replication and load balancing, sharding, security considerations, managing backup strategies, and more.  Database administrators are typically expected to have expertise in database design and theory and be able to help organizations make decisions about database technology selection and implementation.  In many modern organizations, the responsibilities traditionally held by DBAs are now distributed between various members of the development and operations teams or have been offloaded to external providers to simplify some of the infrastructure management portions of the job.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="database-engine"><
    strong>Database engine</strong></dt><dd>A database engine is the piece of a database management system responsible for defining how data is stored and retrieved, as well as the actions supported for interacting with the system and data.  Some database management systems support multiple database engines that offer different features and designs, while other systems only support a single database engine that has been designed to align with the goals of the software.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="database-management-system"><
    strong>Database management system (DBMS)</strong></dt><dd>A database management system, often called a DBMS or even just a &quot;database&quot;, is an application responsible for organizing and managing data.  DBMSs can follow many different paradigms and prioritize certain goals.  Generally, at the very least, they are responsible for persisting data, organizing and categorizing data, and ingesting, manipulating, and querying data.  Most often, DBMSs offer a client / server model where the server is responsible for controlling and managing the data while clients, libraries, or APIs can be used to interact with the server to add or query data, change data structures, or manage other aspects of the system.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="database-model"><
    strong>Database model</strong></dt><dd>A database model is the overall strategy used by a database management system for storing, organizing, and providing access to data.  There are many different database models available, but the relational model, which uses highly structured tables to store data in a specific format, is perhaps the most common type.  Other types of databases include document databases, wide-column databases, hierarchical databases, key-value stores, and more.  Some database systems are designed to be &quot;multi-model&quot;, meaning they support databases with different types of models running within the same system.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="database-proxy"><
    strong>Database proxy</strong></dt><dd>A database proxy is a software component responsible for managing connections between database clients and database servers.  Database proxies are used for a number of reasons including organizing access to a limited number of connections, allowing transparent scaling of the database layer, and redirecting traffic for deployments and similar scenarios.  Database proxies are usually designed to be transparent for applications, meaning that the applications can connect to the proxy as if they were connecting directly to the backend database.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="dataset"><
    strong>Dataset</strong></dt><dd>A dataset, sometimes spelled data set, is a single collection of data.  Typically, this represents a chunk of related data applicable to a certain task, application, or area of concern.  Typically, datasets are a combination of the data itself as well as the structure and context necessary to interpret it.  They often consist of a combination of quantitative and qualitative values that can act as the raw data for further analysis and interpretation.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="denormalization"><
    strong>Denormalization</strong></dt><dd>Denormalization is a process where the data and structure within a database is &quot;denormalized&quot; or taken out of a normalized state.  This can happen accidentally if a data structure that is intended to be normalized is ill defined or mismanaged.  However, it is often also performed intentionally in certain scenarios.  Denormalization tends to allow faster access to data by storing values redundantly in different places.  The drawback of this is that write performance suffers and there is a possibility that data can get out of sync since multiple locations are used to represent the same data.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="dirty-read"><
    strong>Dirty read</strong></dt><dd>A dirty read is a specific type of anomaly that can occur where one transaction can read data that hasn&#x27;t been committed by another transaction.  If the second transaction is rolled back instead of committed, the first transaction will be using a value that doesn&#x27;t reflect the actual state of the database.  Dirty reads are possible at certain isolation levels for transactions and represent a risk that can lead to inconsistency when manipulating data in parallel.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="distributed-database"><
    strong>Distributed database</strong></dt><dd>A distributed database is a database system that spans multiple physical systems.  Data is spread across a number of machines for the sake of performance or availability.  While distributed systems can help scale a database to handle more load, they also represent a significant increase in complexity that can lead to consistency and partition challenges as well as certain negative performance impacts like an increase in data writes in some cases.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="document"><
    strong>Durability</strong></dt><dd>Durability is a quality of data that signifies that it has been captured on persistent storage that will survive in the event of a program crash.  Typically, this means flushing the data to a non-volatile storage medium like a hard drive that doesn&#x27;t require electricity to maintain state.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="encoding"><
    strong>Encoding</strong></dt><dd>Encoding is a system that translates between a character system that can represents the components used in written language and a digital representation that the computer can store and operate on.  Different encoding systems have been developed with a wide variety of character ranges.  Some are targeted at specific languages or language families (like ASCII) while others attempt to provide representation for much larger character sets appropriate for different many languages (like the UTF unicode varieties).</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="encrypted-transport"><
    strong>Encrypted transport</strong></dt><dd>Encrypted transport is any type of communication process that encrypts its messages prior to sending them to the recipient.  Transport encryption is necessary to ensure privacy (prevent others from seeing sensitive information) as well as avoid tampering (making manipulation of the data obvious).  Many different encrypted transport systems can used when deploying databases, including TLS/SSL encryption, VPNs, and private networks.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="ephemerality"><
    strong>Ephemerality</strong></dt><dd>Ephemerality is a characteristic that indicates that a piece of data or circumstance is not permanent.  In many ways, it is the opposite of durability.  In databases, certain items, like data you wish to persist, should not be ephemeral.  However, other components, like a secret key used to encrypt a connection between a database and client, can benefit from being ephemeral by preventing key leakage from effecting future or past sessions.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="ephemeral-storage"><
    strong>Ephemeral storage</strong></dt><dd>Ephemeral storage, also sometimes called volatile or non-durable storage, is any storage medium that persists for a short time, often associated with certain conditions.  For instance, in applications, data being stored in memory will only survive while the process is running.  Similarly, data stored to a temporary directory is only available until the system reboots.  Often, ephemeral storage is useful for temporary data or as a holding area before data can be stored on a more permanent medium.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="eventual-consistency"><
    strong>Eventual consistency</strong></dt><dd>Eventual consistency is a description of a consistency / availability strategy implemented by certain distributed computing or database systems.  The CAP theorem of distributed systems states that systems must choose whether prioritize availability or data consistency in the face of a network partition.  Eventual consistent systems make the choice to favor availability by continuing to serve requests even if the server&#x27;s peers are not available to confirm operations.  Eventually, when the partition is resolved, a consistency routine will run to decide on the most correct state of any inconsistent data, but there will be a time where the data on different servers are not in agreement.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="eviction"><
    strong>Eviction</strong></dt><dd>In the context of caches, eviction is a process where a piece of data is removed from a cache.  This can happen because the current value has been invalidated by an operation or it can occur automatically as a result of policies designed to remove the data that is the oldest or least used.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="expand-and-contract-pattern"><
    strong>Expand and contract pattern</strong></dt><dd>The expand and contract pattern is a strategy for introducing new changes to a database schema without affecting existing applications.  It works by introducing changes in carefully controlled stages by first adding new or changed structures alongside existing structures and then expanding the application logic to use both structures simultaneously.  Eventually, after testing, the application can stop writing to original structure and it can be removed.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="extract-transform-load"><
    strong>Extract-transform-load (ETL)</strong></dt><dd>Extract, transform, and load, often abbreviated as ETL, is a process of copying and processing data from a data source to a managed system.  First the data is extracted from its current system to make it accessible to the destination system.  Next, the data is manipulated and modified to match the requirements and format of the new system.  Finally, the reconstructed data is loaded into the new system.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="feature-flags"><
    strong>Feature flags</strong></dt><dd>A feature flag, or a feature toggle, is a programming strategy that involves gating functionality behind an external switch or control.  The switch is typically first set to indicate that the feature should not be active.  When the organization is ready, they can activate the switch and the program will start using its new functionality.  This allows new features to be deployed without immediately activating them.  It decouples the deployment of new software from the release of the software, offering greater control over how a change is introduced and for greater testing in a production environment.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="field"><
    strong>Flat-file database</strong></dt><dd>A flat-file database is a database or database-like structure stored in a file.  These define the structure and the data the database contains in a unified format.  Many examples of flat-file databases, like CSV (comma-separated values) files are written in plain text, but binary formats exist too.  One difference between flat-file databases and more complex types is that the storage format itself often is responsible for describing the relationships between data instead of the database system.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="foreign-key"><
    strong>Foreign key</strong></dt><dd>A foreign key is a designated column or group of columns in a relational database that is used to maintain data integrity between two tables.  A foreign key in one table refers to a candidate key, typically the primary key, in another table.  Since a candidate key is referenced, each row in the database will be unique and the two tables can be linked together row for row.  The values are of these designated columns is expected to remain identical across the two tables.  The foreign key constraint allows the database system to enforce this requirement by not allowing the values to be out of sync.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="full-text-search"><
    strong>Hierarchical database</strong></dt><dd>A hierarchical database is a database model that organizes itself into a tree-like structure.  Each new record is attached to a single parent record.  As records are added to the database, a tree-like structure emerges as records fan out more and more from the root record.  The links between records can be traversed to get to other records.  Examples of systems that use a hierarchical model include LDAP (Lightweight Directory Access Protocol) and DNS (Domain Name System).</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="horizontal-scaling"><
    strong>Hot backup</strong></dt><dd>A hot backup is a backup of a database system while it is actively in use.  They are often preferable, if possible, because they do not require the database system to be taken offline to perform the operation.  Hot backups are not always possible as they can require locking certain parts of the database or can reduce the IOPS (Input / Output Operations per Second) available for normal database tasks.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="in-memory-database"><
    strong>Ingesting</strong></dt><dd>Ingesting data is the act of importing new data into a data system.  This can be a one-off data loading operation or a continuous consumption of data being generated by other system.  Data ingestion is a common stage of populating and updating analytic databases and big data stores as they often involve consolidating data from various sources.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="inner-join"><
    strong>Inner join</strong></dt><dd>An inner join is a type of relational database operation that joins two tables by only returning rows where the joining column values exist in both tables.  With an inner join, there must be a match on the join columns in both tables.  There are no rows using `NULL` values to pad out rows missing from one table or the other.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="interactive-transactions"><
    strong>Interactive transactions</strong></dt><dd>Interactive transactions are a database transaction feature that allows clients to manually specify transaction operations in an ad-hoc manner.  Rather than a transaction being a wrapper around a group of queries that can all be executed sequentially with no pause, interactive transactions allow developers to briefly pause their database operations to execute other logic before continuing with the transaction processing.  This gives flexibility in transaction processing but can lead to unwanted transaction running times if not carefully managed.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="isolation"><
    strong>Isolation</strong></dt><dd>In the context of databases, isolation is a property that describes how data and operations are visible within and between transactions.  The level of isolation can be set by the database administrator or the query author to define the trade-offs between isolation levels and performance.  Isolation is one of the key guarantees described by the ACID acronym.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="isolation-levels"><
    strong>Isolation levels</strong></dt><dd>Isolation levels describe different types of trade-offs between isolation and performance that databases can make when processing transactions.  Isolation levels determine what types of data leaking can occur between transactions or what data anomalies can occur.  In general, greater levels of isolation provide more guarantees at the expense of slower processing.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="join"><
    strong>Left join</strong></dt><dd>A left join is a join operation for relational databases where all of the rows of the first table specified are returned, regardless of whether a matching row in the second table is found.  Join operations construct virtual rows by matching records that have identical values in specified comparison columns from each table.  The results for a left join will contain the rows from both tables where the column values matched and will additionally contain all of the unmatched rows from the first, or left, table.  For these rows, the columns associated with the second, or right, table will be padded with `NULL` values to indicate that no matching row was found.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="lexeme"><
    strong>Lexeme</strong></dt><dd>Lexemes are language-level units of meaning that are relevant in natural language processing and full-text search contexts.  Typically, when text is indexed, it is broken down into individual tokens which are then analyzed as lexemes using language-level resources like dictionaries, thesauruses, and other word lists to understand how to process them further.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="locale"><
    strong>Locale</strong></dt><dd>In databases and computing in general, a locale specifies the region, language, country, and other pieces of contextual data that should be used when performing operations and rendering results.  In databases, locale settings can affect things like column orderings, comparisons between values, spelling, currency identifiers, date and time formatting, and more.  Defining the correct locale at the database server level or requesting the locale you need during a database session are essential for ensuring that the operations are performed will yield the expected results.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="lock"><
    strong>Migration (database, schema)</strong></dt><dd>Database or schema migrations are processes used to transform a database structure to a new design.  This involves operations to modify the existing schema of a database or table as well as transforming any existing data to fit the new structure.  Database migrations are often built upon one another and stored as an ordered list in version control so that the current database structure can be built from any previous version by sequentially applying the migration files.  Often, developers must make decisions about how best to modify existing data to fit the new structure which might include columns that did not previously exist or changes to data that are difficult to easily reverse.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="mongodb"><
    strong>Multiversion concurrency control (MVCC)</strong></dt><dd>Multiversion concurrency control, or MVCC, is a strategy for allowing concurrent access to data within database systems as an alternative to row and table locking.  MVCC works by taking &quot;snapshots&quot; that represent a consistent data state for each user accessing a set of data.  The goal of MVCC is to offer a system where read queries never block write queries and where write queries never block read queries.  Each client will be able to read and use the data as if they were the only user while the database system tracks multiple versions of the data being read and updated by each user.  Locking or the normal transaction rollback and conflict management strategies are used to resolve disputes caused by updating the same data.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="mysql"><
    strong>Nonrepeatable read</strong></dt><dd>A nonrepeatable read is a type of unwanted consistency problem that can occur at certain transaction isolation levels.  A nonrepeatable read occurs when a repeated read operations within a transaction can return different data based based on commits outside of the transaction.  This breach of isolation is one of the types of behavior that some transaction isolation levels are designed to prevent.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="normalization"><
    strong>Normalization</strong></dt><dd>Database normalization is a process of structuring a database to remove data redundancy and eliminate opportunities for inconsistencies to be introduced.  Normalization is often discussed in terms of &quot;normal forms&quot; where each form adds additional checks and guarantees over the previous forms.  In practice, data normalization is often a trade-off between data integrity guarantees and performance, so structures often are not put into the highest level of normalization possible.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="olap-database"><
    strong>Object relational impedance mismatch</strong></dt><dd>Object relational impedance mismatch is a term used for the general tension that exists between the relational model of data used by many databases and the object-oriented view of data used in many applications.  The impedance mismatch refers to the differences between the two models that makes faithful translation between the representations difficult or impossible.  It is a broad term used to refer to many different types of problems that can occur within the space including problems representing inheritance, encapsulation, type differences, different consistency guarantees, and more.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="optimistic-concurrency-control"><
    strong>Optimistic concurrency control</strong></dt><dd>Optimistic concurrency control, sometimes referred to as OCC, is a strategy used by database systems to handle conflicting concurrent operations.  Optimistic concurrency control assumes that concurrent transactions will likely not interfere with each other and allows them to proceed.  If a conflict occurs when a transaction attempts to commit, it will be rolled back at that time.  OCC is an attractive policy if you think that most transactions within your workloads will not be in conflict with one another.  Only transactions that do in fact have a conflict will suffer a performance penalty (they&#x27;ll be rolled back and will have to be restarted) while all non-conflicting transactions can execute without waiting to see if a conflict will arise.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="outer-join"><
    strong>Outer join</strong></dt><dd>An outer join is a type of relational database operation that joins two tables by returning all rows from each component table, even where there is not a matching record in the companion table.  Join operations construct virtual rows by matching records that have identical values in specified comparison columns from each table.  The results for an outer join will contain the rows from both tables where the column values matched and will additionally contain all of the unmatched rows from each table.  For these rows, the columns without a match in the other table will be padded with `NULL` values to indicate that no matching row was found.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="parameterized-query"><
    strong>Parameterized query</strong></dt><dd>A parameterized query, also known as a prepared statement, is a database query that has been to take user input as parameters instead of by concatenating strings with user input.  Parameterized queries allow you to specify the query, including the unknown inputs ahead of time and then later provide the values that should be substituted within the statement.  This prevents SQL injection vulnerabilities where carefully crafted inputs can be used to make the database system misinterpret a query by viewing values as executable SQL code.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="persistence"><
    strong>Persistence</strong></dt><dd>Persistence is a quality of data that indicates that the state will outlive the process that created it.  Persistence is a key part of most database systems and allows the data to be loaded once again after the database process or the server itself is restarted.  Applications and databases can have various levels of persistence that guard against different types of failure conditions like single system persistence, remote persistence, and cluster persistence.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="persistent-storage"><
    strong>Persistent storage</strong></dt><dd>Persistent storage refers to any storage medium that is able to maintain data after the system loses power or is disconnected.  Persistent storage is required to maintain a more permanent repository of data.  Often, persistent storage is slower than ephemeral storage like in-memory data, so database systems use a variety of processes to shuttle data between the two storage systems as needed to take advantage of and balance the disadvantages of both types.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="pessimistic-concurrency-control"><
    strong>Pessimistic concurrency control</strong></dt><dd>Pessimistic concurrency control, or PCC, is a strategy used by database systems to handle conflicting concurrent operations.  In contrast to optimistic concurrency control, pessimistic concurrency control short circuits transactions as soon as the possibility of a conflict arises.  This strategy is useful if frequent conflicts occur because it ensures that the system does not waste time executing transactions that will be unable to commit due to conflict.  Instead, it enforces a more serialized execution approach when conflicts might occur, which is slower, but avoids non-productive processing.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="phantom-read"><
    strong>Phantom read</strong></dt><dd>A phantom read is a type of isolation anomaly that can occur within a transaction under certain types of isolation levels.  A phantom read occurs when different rows are returned for a `SELECT` operation during a transaction due to changes made outside of the transaction.  For example if you try to `SELECT` all records in a table, the first time it could return 8 rows, but if another transaction commits an additional row, a repeat of the original query would show a different result.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="postgresql"><
    strong>Precision (searching)</strong></dt><dd>In the context of search performance, precision is a measure of how relevant the retrieved results are to the given query.  Specifically, search precision is defined as the ratio between the number of relevant results out of all of the results that were returned.  A query with a high level of precision does not retrieve many items that are not applicable to the query.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="primary-key"><
    strong>Primary key</strong></dt><dd>A primary key is a type of database key that is designated as the main way to uniquely address a database row.  While other keys may be able to pull individual rows, the primary key is specifically marked for this purpose with the system enforcing uniqueness and not `NULL` consistency checks.  A primary key can be a natural key (a key that is naturally unique across records) or a surrogate key (a key added specifically to serve as a primary key) and can be formed from a single or multiple columns.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="query"><
    strong>Query</strong></dt><dd>In databases, a query is a formatted command used to make a request to a database management system using a query language.  The database system processes the query to understand what actions to take and what data to return to the client.  Often, queries are used to request data matching specific patterns, insert new data into an existing structure, or modify and save changes to existing records.  In addition to targeting data items, queries can often manipulate items like table structures and the server settings, making them the general administrative interface for the system.  SQL, or Structured Query Language, is the most common database querying language used with relational databases.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="query-builder"><
    strong>Query language</strong></dt><dd>A query language is a type of programming language that specializes in searching for, retrieving, and manipulating data in databases.  SQL, or Structured Query Language, is the most common querying language in the world and is used primarily to manage data within relational database systems.  Query language operations can be categorized based on the focus and target of the procedure into Data Definition Language (DDL) when they are used to define data structures, Data Control Language (DCL) when they are used for system management tasks, and Data Manipulation Language (DML) when they are used to modify data.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="query-planner"><
    strong>Query planner</strong></dt><dd>A query planner is an internal component of a database system that is responsible for translating a client provided query into steps that can be used to actually search the database and construct the desired response.  Well designed query planners can consider multiple potential solutions and select the option that will give the most optimized results.  Sometimes, query planners do not select the best solution and database administrators must tweak the selection criteria manually.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="raft-consensus-algorithm"><
    strong>Raft consensus algorithm</strong></dt><dd>The Raft consensus algorithm is an algorithm designed to coordinate information sharing, management responsibilities, and fault recovery across a cluster of nodes.  The algorithm provides a method to ensure that each member agrees on data operations and includes mechanisms for leader election in cases of network partitions or node outages.  It is generally considered a simpler algorithm to implement than alternatives like <a href="https://en.wikipedia.org/wiki/Paxos_(computer_science)">Paxos</a>.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="read-committed-isolation-level"><
    strong>Read committed isolation level</strong></dt><dd>The read committed isolation level is a transaction isolation level for relational database systems that offers a minimal amount of isolation guarantees.  At the read committed level, transactions are guaranteed to be free of dirty reads, a phenomena where transactions can read data from other transactions that have not been committed yet.  Nonrepeatable reads, phantom reads, and serialization anomalies are still possible at this isolation level.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="read-operation"><
    strong>Read-through caching</strong></dt><dd>Read-through caching is a caching strategy where the cache is deployed in the path to the backing data source.  The application sends all read queries directly to the cache.  If the cache contains the requested item, it is returned immediately.  It the cache request misses, the cache fetches the data from the backing database in order to return the items to the client and add it to the cache for future queries.  In this architecture, the application continues to send all write queries directly to the backing database.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="read-uncommitted-isolation-level"><
    strong>Read uncommitted isolation level</strong></dt><dd>The read uncommitted isolation level is a transaction isolation level for relational database systems that fundamentally offers no isolation.  Transactions performed using the read uncommitted isolation level can suffer from dirty reads, nonrepeatable reads, phantom reads, and serialization anomalies.  Generally speaking, the read uncommitted level is not very useful as it does not fulfill most users&#x27; expectations for isolation.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="recall"><
    strong>Recall</strong></dt><dd>In the context of search performance, recall is a measure of how many of the relevant items a query was able to retrieve.  Recall is specifically defined as the ratio of the number of relevant results returned by a query compared to the total number of relevant entries in the dataset.  A query with high recall retrieves a large number of the items that would be potentially relevant to a search query.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="record"><
    strong>Repeatable read isolation level</strong></dt><dd>The repeatable read isolation level is a transaction isolation level for relational database systems that offers better isolation than read committed level, but not as much isolation of the serializable level.  At the repeatable read isolation level, dirty reads and nonrepeatable reads are both prevented.  However, phantom reads and serialization anomalies can still occur.  This means that while reads of individual records are guaranteed to remain stable, range queries (like `SELECT` statements that return multiple rows) can change as a result of commits outside of the transaction.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="replication"><
    strong>Right join</strong></dt><dd>A right join is a join operation for relational databases where all of the rows of the second table specified are returned, regardless of whether a matching row in the first table is found.  Join operations construct virtual rows by matching records that have identical values in specified comparison columns from each table.  The results for a right join will contain the rows from both tables where the column values matched and will additionally contain all of the unmatched rows from the second, or right, table.  For these rows, the columns associated with the first, or left, table will be padded with `NULL` values to indicate that no matching row was found.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="role-based-access-control"><
    strong>Role-based access control (RBAC)</strong></dt><dd>Role-based access control, also known as RBAC, is a security strategy that restricts the operations permitted to a user based on their assigned roles.  Permissions on object and privileges to execute actions are assigned to roles, labels that make managing access easier.  To grant the capabilities associated with a role to a user, the user can be made a member of the role.  Users can be made a member of multiple roles to gain a union of the permissions each role provides.  Roles are helpful as a way of standardizing the privileges required for various roles and making it simple to add or remove access to users.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="row"><
    strong>Serial scanning</strong></dt><dd>Serial scanning is a search technique that involves analyzing each potential item against the query at the time of the search.  This is in opposition to index-based searching where items are accounted for and organized ahead of time to allow for faster query response.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="sql"><
    strong>Serializable isolation level</strong></dt><dd>The serializable isolation level is a transaction isolation level for relational database systems that offers the strictest isolation guarantees.  At the serializable level, dirty reads, nonrepeatable reads, phantom reads, and serialization anomalies are all prevented.  The database system does this by aborting any transactions where conflicts may occur, which ensures that concurrent transactions can be applied as if they were serially applied.  Serializable isolation provides 
      strong isolation, but it can suffer from significant performance problems due to the fact that conflicting transactions may be aborted and have to be resubmitted.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="serialization-anomaly"><
    strong>Serialization anomaly</strong></dt><dd>A serialization anomaly is a problem that can occur with concurrent transactions where the order that concurrent transactions are committed can impact the resulting data.  Serialization anomalies occur because operations in different transactions can be making calculations based on data that other transactions may be updating.  To prevent serialization anomalies, transactions must use the serializable isolation level, which prevents these conditions by rolling back one of the conflicting transactions.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="shard"><
    strong>Shard</strong></dt><dd>A database shard is a segment of records stored by a database object that is separated out and managed by a different database node for performance reasons.  For example, a database table with 9 million records could be divided into three separate shards, each managing 3 million records.  The data is typically divided according to a &quot;shard key&quot; which is a key that determines which shard a record should be managed by.  Each shard manages its subset of records and a coordinating component is required to direct client queries to the appropriate shard by referring to the shard key.  Sharding can help some types of performance in very large datasets but it often requires making trade-offs that might degrade other types of performance (for instance, on operations that need to coordinate between multiple shards).</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="stale-data"><
    strong>Stale data</strong></dt><dd>When working with data storage, stale data refers to any data that does not accurately reflect the most recent state of the data.  This is often a concern primarily when caching, as pieces of data might potentially be preserved and used long after they been invalidated by changes.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="standard-column-family"><
    strong>Standard column family</strong></dt><dd>A standard column family is a type of column family database object that stores data by defining row keys that are associated with key value pairs akin to columns.  Each row can define and use its own columns, so the resulting dataset is not regular as with relational database tables.  However, the row keys combined with column labels and values still somewhat resembles a table.  Standard column families offer good performance for key-based data retrieval as they are able to store all of the information associated with a key in the same place and can modify the data structure for that key easily.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="stemming"><
    strong>Stemming</strong></dt><dd>Stemming is a technique used in full-text search indexing where words with the same stem are collapsed into a single entry.  This increases the number of relevant results considered at the expense of a slight decrease in precision.  For instance, the words &quot;cook&quot;, &quot;cooked&quot;, and &quot;cooks&quot; might occupy a single entry where a search for any of the terms would return results for the whole entry.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="stop-words"><
    strong>Stop words</strong></dt><dd>In full-text search contexts, stop words are a list of words that are considered inapplicable to search queries.  These are typically the most common words in a language that lack much meaning on their own or are ambiguous to the point of irrelevancy.  Some examples in English are words like &quot;the&quot;, &quot;it&quot;, and &quot;a&quot;.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="storage-engine"><
    strong>Storage engine</strong></dt><dd>A storage engine is the underlying component in database management systems that is responsible for inserting, removing, querying, and updating data within the database.  Many database features, like the ability to execute transactions, are actually properties of the underlying storage engine.  Some database systems, like MySQL, have many different storage engines that can be used according to the requirements of your use case.  Other systems, like PostgreSQL, focus on providing a single storage engine that is useful in all typical scenarios.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="stored-procedure"><
    strong>Stored procedure</strong></dt><dd>A stored procedure is a way to define a set of operations within the database that clients can easily run.  Because they are stored within the database, they can sometimes offer performance improvements and avoid network latency.  Stored procedures differ from user defined functions in that they must be explicitly invoked with a special statement rather than incorporated into other queries and cannot be used in all of the same scenarios.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="super-column-family"><
    strong>Super column family</strong></dt><dd>A super column family is a type of column family database object that stores data by defining row keys that are associated with column families. Each row can contain multiple column families as a way of segmenting data further than in standard column families.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="superkey"><
    strong>Superkey</strong></dt><dd>A superkey is any set of attributes within the relational database model that can be used to uniquely identify a record.  All other key types (primary keys, candidate keys, composite keys, etc.) are examples of super keys.  A trivial superkey contains all available attributes, while a candidate key is any superkey that cannot be simplified by removing additional columns.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="table"><
    strong>Table aliases</strong></dt><dd>A table alias is name given at query time for an existing or calculated table or table-like database object.  Table aliases can be useful if the original name is long or ambiguous or if the table is generated by the query itself and requires a label to refer back to it in other parts of the query or for display.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="three-tier-architecture"><
    strong>Three tier architecture</strong></dt><dd>The three tier application architecture is a common infrastructure architecture for deploying web applications.  The first layer is comprised of one or more web servers that respond to client requests, serve static content, and generate requests to the subsequent layers.  The second layer is handled by application servers and is responsible for generating dynamic content by executing code to generate responses for the front end.  The third layer is handled by the database system and is responsible for responding to requests from the middle layer for custom values used to generate content.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="token"><
    strong>Two-phase commit</strong></dt><dd>Two-phase commit is an algorithm used to implement transactions in distributed systems.  Two-phase commits work by separating the commit process into two general stages.  In the first stage, a potential change is communicated by the server that received it to a coordinating component.  The coordinator requests a vote from all of the involved servers on whether to commit or not.  If the vote succeeds, the second stage begins where the transaction is actually committed by each individual member.  The algorithm allows distributed systems to maintain a consistent dataset at the expense of the overhead associated with coordinating the voting procedure.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="two-phase-locking"><
    strong>Two-phase locking</strong></dt><dd>Two-phase locking, sometimes abbreviated as 2PL, is a strategy for concurrency control to ensure that transactions are serializable.  The two phases refer to actions that expand the number of locks held by the transaction and the actions that trigger a release of locks.  Two phase locking works by using exclusive and shared locks to coordinate read and write operations.  A transaction that needs to read data can request a shared read lock that allows other transactions to read the same data but blocks write operations.  Because this is a shared lock, each successive read operation can simultaneously request a read lock and the data will remain unmodifiable until they are all released.  A transaction that needs to modify data requests an exclusive write lock which prevents other write locks and any read locks from being issued.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="upsert"><
    strong>Upsert</strong></dt><dd>An upsert is a database operation that either updates an existing entry or inserts a new entry when no current entry is found.  Upsert operations consists of a querying component that is used to search for matching records to update and a mutation component that specifies which values should be updated.  Often, additional values need to be provided for other fields to handle the case where a new record must be created.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="value"><
    strong>Value</strong></dt><dd>When talking about databases, a value is any piece of data that the database system stores within its data structures.  With additional context like the name of the field where the value is stored, meaning can be assigned to the value beyond what is intrinsically there.   The specific storage structure like the column or table may define requirements about what types of values it stores.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="vertical-scaling"><
    strong>Vertices</strong></dt><dd>In graph databases, vertices are entities that can hold properties and be connected to other vertices through edges.  Vertices are similar to a record or a document in other database systems as they have a label or name to indicate the type of object they represent and they have attributes that provide specific additional information to differentiate a specific vertex from others of its type.  Vertices are connected to other vertices through edges that define a relationship between them.  For instance, an &quot;author&quot; vertex can be connected to a &quot;book&quot; vertex with a &quot;written by&quot; edge.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="view"><
    strong>View</strong></dt><dd>In relational databases, a view is a table-like representation of a stored query.  Views can be used as tables in many contexts, but instead of being part of the underlying data structure, they are derived from the results of their query.  Views are useful for constructing more complex representations of data than exists in the underlying schema.  For example, a view might join a few tables and display only a few relevant columns, which can help make the data more useable even if a different structure is preferable for storage due to consistency or performance reasons.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="volatile-storage"><
    strong>Volatile storage</strong></dt><dd>Volatile storage is any type of storage that is dependent on continual power to persist data.  For example, data stored in RAM is typically considered to be volatile because it will be lost and unrecoverable in the event of a power outage.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="wide-column-store"><
    strong>Wide-column store</strong></dt><dd>A wide-column store is a type of NoSQL database that organizes its data into rows and columns using standard and super column families.  A row key is used to retrieve all of the associated columns and super columns.  Each row can contain entirely different columns as the column definitions and values are stored within the row structure itself.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="wal"><
    strong>Write-ahead logging (WAL)</strong></dt><dd>Write-ahead logging, or WAL, is an approach to data revision management that increases the resiliency of systems data corruption during crashes and failures.  Without a technique like WAL, corruption can occur if the system crashes when a change to a database is only partially completed.  In this case, the data will be in neither the initial nor the intended state.  With write ahead logging, the system records its intentions to a durable write ahead log before executing operations.  This way, the database can recover a known-good state of the data by reviewing the log during recovery and redoing any operations that did not complete correctly initially.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="search-weight"><
    strong>Weight (search)</strong></dt><dd>In the context of searching, a search weight is an arbitrary value assigned to different categories of data designed to influence the priority of the item when analyzed for relevance.  Assigning a heavy weight to a specific type of information will cause a query engine to assign greater significance to that category compared to other categories when compiling a list of relevant results.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="write-around-caching"><
    strong>Write-around caching</strong></dt><dd>Write-around caching is a caching pattern where write queries are sent to the backing database directly rather than written to the cache first.  Because any items in the cache related to the update will be now be stale, this method requires a way to invalidate the cache results for those items for subsequent reads.  This technique is almost always combined with a policy for cache reads to control read behavior.  This approach is best for data that is read infrequently once written or updated.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="write-back-caching"><
    strong>Write-back caching</strong></dt><dd>Write-back caching is a caching method where write queries are sent to the cache instead of the backing database.  The cache then periodically bundles the write operations and sends them to the backing database for persistence.  This is a modification of the write-through caching approach to reduce strain caused by high throughput write operations at the cost of less durability in the event of a crash.  This ensures that all recently written data is immediately available to applications without additional operations, but can result in data loss if the cache crashes before it&#x27;s able to persist writes to the database.</dd></div><div class="anchor-item__AnchorItemWrapper-sc-1k9lgyc-0 eYMFMY"><dt id="write-operation"><
    strong>Write-through caching</strong></dt><dd>Write-through caching is a caching pattern where the application writes changes directly to the cache instead of the backing database.  The cache then immediately forwards the new data to the backing database for persistence.  This strategy minimizes the risk of data loss in the event of a cache crash while ensuring that read operations have access to all new data.  In high write scenarios, it may make sense to transition to write-back caching to prevent straining the backing database.</dd></div></dl>
